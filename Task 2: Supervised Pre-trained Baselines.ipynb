{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2017694,"sourceType":"datasetVersion","datasetId":1207662}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jfjerin/densenet201?scriptVersionId=278985243\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport os\nfrom PIL import Image\nimport time\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support,\n    confusion_matrix, roc_curve, auc\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- 1. Custom Dataset Class ---\nclass CervicalDataset(Dataset):\n    def __init__(self, root_dir, transform=None, split_type='train'):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.split_type = split_type \n        self.image_paths = []\n        self.labels = []\n        self.label_map = {} \n\n        print(f\"Initializing CervicalDataset for split '{split_type}' from root_dir: {root_dir}\")\n\n        top_level_items = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n        \n        found_images_count = 0\n        \n        if not top_level_items:\n            print(f\"WARNING: No subdirectories found in {root_dir}. Please check your `data_dir`.\")\n            if os.path.isdir(os.path.join(root_dir, 'im_Dyskeratotic')):\n                top_level_items = ['im_Dyskeratotic']\n                print(f\"Inferred 'im_Dyskeratotic' as top-level item in {root_dir}.\")\n            else:\n                print(f\"Could not infer top-level class folders. Please ensure `data_dir` points to the root of your dataset.\")\n\n\n        for top_item in top_level_items:\n            first_level_path = os.path.join(root_dir, top_item)\n            \n            # Check for the nested class folder (e.g., 'im_Dyskeratotic' inside 'im_Dyskeratotic')\n            nested_class_path = os.path.join(first_level_path, top_item) \n            \n            # Check for the 'CROPPED' folder within the nested path\n            cropped_folder_path = os.path.join(nested_class_path, 'CROPPED')\n\n            if os.path.isdir(cropped_folder_path):\n                print(f\"Found CROPPED folder for class '{top_item}' at: {cropped_folder_path}\")\n                for img_name in os.listdir(cropped_folder_path):\n                    if img_name.lower().endswith('.bmp'):\n                        img_path = os.path.join(cropped_folder_path, img_name)\n                        self.image_paths.append(img_path)\n                        self.labels.append(top_item) \n                        found_images_count += 1\n            else:\n                # Fallback: Check if CROPPED is directly under the first_level_path (if no nesting)\n                direct_cropped_path = os.path.join(first_level_path, 'CROPPED')\n                if os.path.isdir(direct_cropped_path):\n                    print(f\"WARNING: Found CROPPED directly under {first_level_path} (no nested class folder).\")\n                    for img_name in os.listdir(direct_cropped_path):\n                        if img_name.lower().endswith('.bmp'):\n                            img_path = os.path.join(direct_cropped_path, img_name)\n                            self.image_paths.append(img_path)\n                            self.labels.append(top_item)\n                            found_images_count += 1\n                else:\n                    print(f\"Neither nested 'CROPPED' nor direct 'CROPPED' found for '{top_item}' at {first_level_path}. Skipping.\")\n        \n        if found_images_count == 0:\n            raise ValueError(f\"No .bmp images found in the specified root directory: {root_dir}. \"\n                             f\"Please check your `data_dir` path and the exact dataset structure.\")\n\n        print(f\"Total images found for split '{split_type}': {len(self.image_paths)}\")\n\n        unique_labels = sorted(list(set(self.labels)))\n        self.label_map = {label: i for i, label in enumerate(unique_labels)}\n        self.int_labels = [self.label_map[label] for label in self.labels]\n        self.num_classes = len(self.label_map)\n        \n        print(f\"Detected classes: {unique_labels}\")\n        print(f\"Label map: {self.label_map}, Number of classes: {self.num_classes}\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n            \n        label = self.int_labels[idx]\n        return image, label\n\n# Custom Dataset class to handle pre-split paths and labels\nclass SplitDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# --- 2. Data Preprocessing and Loading ---\n\n# Define transformations\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet stats\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = '/kaggle/input/cervical-cancer-largest-dataset-sipakmed'\n\nprint(f\"\\nAttempting to load data from: {data_dir}\")\nif not os.path.exists(data_dir):\n    raise FileNotFoundError(f\"The specified data_dir does not exist: {data_dir}. Please check the path.\")\n\n# Create a \"master\" dataset to get all paths and labels first\nmaster_dataset = CervicalDataset(root_dir=data_dir, transform=None, split_type='master')\nnum_classes = master_dataset.num_classes\nclass_names = list(master_dataset.label_map.keys())\n\nprint(f\"\\nMaster dataset size: {len(master_dataset)} images.\")\nprint(f\"Number of classes detected: {num_classes}\")\n\n# Extract all image paths and their integer labels\nall_image_paths = master_dataset.image_paths\nall_int_labels = master_dataset.int_labels\n\n# Perform stratified train/validation/test split\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    all_image_paths, all_int_labels, test_size=0.15, stratify=all_int_labels, random_state=42\n)\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    train_paths, train_labels, test_size=(0.15/0.85), stratify=train_labels, random_state=42\n)\n\nprint(f\"Dataset split sizes: Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")\n\n# Create dataset instances for each split with their specific transforms\ntrain_dataset = SplitDataset(train_paths, train_labels, data_transforms['train'])\nval_dataset = SplitDataset(val_paths, val_labels, data_transforms['val'])\ntest_dataset = SplitDataset(test_paths, test_labels, data_transforms['test'])\n\n# Check for empty datasets after splitting\nif len(train_dataset) == 0 or len(val_dataset) == 0 or len(test_dataset) == 0:\n    raise ValueError(\"One or more dataset splits resulted in zero size. \"\n                     \"Ensure your `master_dataset` is large enough for splitting.\")\n\n# Create DataLoaders\ndataloaders = {\n    'train': DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2), # num_workers can be adjusted\n    'val': DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2),\n    'test': DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2),\n}\n\nprint(\"\\nDataLoaders created successfully with the following dataset sizes:\")\nprint(f\"Train Dataset: {len(train_dataset)} images\")\nprint(f\"Validation Dataset: {len(val_dataset)} images\")\nprint(f\"Test Dataset: {len(test_dataset)} images\")\nprint(f\"Train DataLoader batches: {len(dataloaders['train'])}\")\nprint(f\"Validation DataLoader batches: {len(dataloaders['val'])}\")\nprint(f\"Test DataLoader batches: {len(dataloaders['test'])}\")\n\n# --- 3. Model Selection and Loading (DenseNet201 Architecture) ---\n\n# Load pre-trained DenseNet201\n# Using 'weights' parameter for recommended practice (instead of deprecated 'pretrained=True')\nmodel_ft = models.densenet201(weights=models.DenseNet201_Weights.IMAGENET1K_V1)\nprint(\"Loaded pre-trained DenseNet201 model.\")\n\n# 4. Freeze initial layers\n# DenseNet's feature extractor is typically everything before the final classifier\nfor param in model_ft.parameters():\n    param.requires_grad = False\nprint(\"Frozen initial layers of DenseNet201.\")\n\n# 5. Modify classification head\n# DenseNet's final classification layer is accessed via `model_ft.classifier`\nnum_ftrs = model_ft.classifier.in_features\nmodel_ft.classifier = nn.Linear(num_ftrs, num_classes)\nprint(f\"Replaced DenseNet201's classifier head with output for {num_classes} classes.\")\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel_ft = model_ft.to(device)\n\nprint(f\"\\nModel initialized and moved to device: {device}\")\n\n# --- 6. Define Loss Function and Optimizer ---\ncriterion = nn.CrossEntropyLoss()\n# Optimize only the parameters of the newly added classification head\n# `filter(lambda p: p.requires_grad, model_ft.parameters())` correctly selects only trainable parameters\noptimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.001)\n\n# --- 7. Training Loop ---\ndef train_model(model, criterion, optimizer, dataloaders, device, num_epochs=10):\n    since = time.time()\n    best_acc = 0.0\n    \n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # Zero the parameter gradients\n                optimizer.zero_grad()\n\n                # Forward\n                # Track gradients only if in training phase\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # Backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accuracies.append(epoch_acc.item())\n            else:\n                val_losses.append(epoch_loss)\n                val_accuracies.append(epoch_acc.item())\n\n            # Deep copy the model if it's the best performing\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n\n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, train_losses, train_accuracies, val_losses, val_accuracies\n    \n\n# --- 8. Evaluation Function ---\ndef evaluate_model(model, dataloader, device, class_names):\n    model.eval() # Set model to evaluate mode\n    all_preds = []\n    all_labels = []\n    all_probs = [] # For ROC/AUC\n\n    start_time = time.time()\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            probs = torch.softmax(outputs, dim=1)\n            _, preds = torch.max(outputs, 1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    end_time = time.time()\n    test_time = end_time - start_time\n\n    # --- Calculate Metrics ---\n    \n    # Overall Accuracy\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    # Precision, Recall, F1-score (weighted average for multi-class)\n    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n    \n    # Per-class metrics\n    class_precision, class_recall, class_f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, zero_division=0)\n    \n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n\n    # ROC Curve and AUC (One-vs-Rest for multi-class)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    # Binarize labels for ROC/AUC\n    binarized_labels = label_binarize(all_labels, classes=range(len(class_names)))\n    \n    for i in range(len(class_names)):\n        if np.sum(binarized_labels[:, i]) == 0: # Skip if no true samples for this class\n            fpr[i], tpr[i], _ = [0], [0], [0]\n            roc_auc[i] = np.nan # Not a number\n            continue\n        \n        # Check if there's only one unique class in the binarized data for this specific class\n        # roc_curve requires at least two unique class labels.\n        if len(np.unique(binarized_labels[:, i])) < 2:\n            print(f\"Warning: ROC curve for class {class_names[i]} cannot be computed (only one class present in true labels).\")\n            fpr[i], tpr[i], _ = [0], [0], [0]\n            roc_auc[i] = np.nan\n            continue\n\n        fpr[i], tpr[i], _ = roc_curve(binarized_labels[:, i], np.array(all_probs)[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n        \n    # Micro-average ROC curve and AUC (good for imbalanced datasets)\n    fpr_micro, tpr_micro, _ = roc_curve(binarized_labels.ravel(), np.array(all_probs).ravel())\n    roc_auc_micro = auc(fpr_micro, tpr_micro)\n\n    # Macro-average ROC curve and AUC\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_names)) if not np.isnan(roc_auc[i])]))\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(len(class_names)):\n        if not np.isnan(roc_auc[i]):\n            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n    mean_tpr /= len([i for i in range(len(class_names)) if not np.isnan(roc_auc[i])]) # Average and normalize\n    fpr_macro = all_fpr\n    tpr_macro = mean_tpr\n    roc_auc_macro = auc(fpr_macro, tpr_macro)\n\n    # --- Report Results ---\n    print(\"\\n--- Evaluation Results ---\")\n    print(f\"Overall Accuracy: {accuracy:.4f}\")\n    print(f\"Precision (weighted): {precision:.4f}\")\n    print(f\"Recall (weighted): {recall:.4f}\")\n    print(f\"F1-Score (weighted): {f1_score:.4f}\")\n    print(f\"Test Inference Time: {test_time:.2f} seconds\")\n\n    print(\"\\nPer-Class Metrics:\")\n    for i, name in enumerate(class_names):\n        print(f\"  Class '{name}' ({i}): Precision={class_precision[i]:.4f}, Recall={class_recall[i]:.4f}, F1={class_f1_score[i]:.4f}\")\n    \n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n\n    print(\"\\nROC AUC:\")\n    for i, name in enumerate(class_names):\n        if not np.isnan(roc_auc[i]):\n            print(f\"  Class '{name}' ({i}) AUC: {roc_auc[i]:.4f}\")\n        else:\n            print(f\"  Class '{name}' ({i}) AUC: Not available (single class present)\")\n    print(f\"  Micro-average AUC: {roc_auc_micro:.4f}\")\n    print(f\"  Macro-average AUC: {roc_auc_macro:.4f}\")\n\n    # --- Visualizations ---\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\n    \n    # Plotting training history\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n    plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n    plt.title('Loss over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(len(train_accuracies)), train_accuracies, label='Train Accuracy')\n    plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy')\n    plt.title('Accuracy over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    # Plot ROC curves\n    plt.figure(figsize=(10, 8))\n    plt.plot(fpr_micro, tpr_micro, label=f'Micro-average ROC (AUC = {roc_auc_micro:.2f})', color='deeppink', linestyle=':', linewidth=4)\n    plt.plot(fpr_macro, tpr_macro, label=f'Macro-average ROC (AUC = {roc_auc_macro:.2f})', color='navy', linestyle=':', linewidth=4)\n    \n    # FIX STARTS HERE\n    # Get a colormap object\n    cmap = plt.colormaps['jet'] \n    # Generate N evenly spaced colors from the colormap\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_names))]\n    # FIX ENDS HERE\n\n    for i, color in zip(range(len(class_names)), colors):\n        if not np.isnan(roc_auc[i]):\n            plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curves')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return accuracy, precision, recall, f1_score, cm, roc_auc, test_time\n\n# --- Main Execution ---\nif __name__ == '__main__':\n    print(\"Starting Cervical Cancer Classification Training with DenseNet201...\")\n    \n    # Train the model\n    # You can adjust num_epochs\n    trained_model, train_losses, train_accuracies, val_losses, val_accuracies = train_model(model_ft, criterion, optimizer_ft, dataloaders, device, num_epochs=10)\n\n    print(\"\\n--- Model Training Complete. Starting Evaluation ---\")\n    \n    # Evaluate the trained model on the test set\n    test_accuracy, test_precision, test_recall, test_f1, test_cm, test_auc, test_time = evaluate_model(\n        trained_model, dataloaders['test'], device, class_names\n    )\n    \n    # If you want a visual of a sample image:\n    print(\"\\n--- Displaying a sample image with its predicted label ---\")\n    \n    # Get a single batch from the test DataLoader\n    sample_inputs, sample_labels = next(iter(dataloaders['test']))\n    \n    # Pick the first image in the batch\n    sample_input = sample_inputs[0].unsqueeze(0).to(device) # Add batch dimension\n    actual_label = sample_labels[0].item()\n    \n    # Get prediction\n    trained_model.eval()\n    with torch.no_grad():\n        output = trained_model(sample_input)\n        _, predicted_class_idx = torch.max(output, 1)\n    \n    predicted_label = predicted_class_idx.item()\n    \n    # Denormalize image for display\n    inv_normalize = transforms.Normalize(\n        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n        std=[1/0.229, 1/0.224, 1/0.225]\n    )\n    display_image = inv_normalize(sample_inputs[0]).permute(1, 2, 0).cpu().numpy()\n    display_image = np.clip(display_image, 0, 1) # Clip to valid range [0, 1]\n\n    plt.figure(figsize=(6, 6))\n    plt.imshow(display_image)\n    plt.title(f\"Actual: {class_names[actual_label]}\\nPredicted: {class_names[predicted_label]}\", fontsize=16)\n    plt.axis('off')\n    plt.show()\n\n    print(\"\\n--- Visualizing the concept of cervical cell classification with DenseNet201 ---\")\n    \n    \n    print(\"\\nThis image conceptually illustrates how a DenseNet201 model, like the one implemented, processes microscopic cervical cell images to classify them, aiding in early detection and diagnosis of cervical cancer.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:20:09.74222Z","iopub.execute_input":"2025-11-17T05:20:09.7426Z","iopub.status.idle":"2025-11-17T05:23:32.779515Z","shell.execute_reply.started":"2025-11-17T05:20:09.742567Z","shell.execute_reply":"2025-11-17T05:23:32.778494Z"}},"outputs":[],"execution_count":null}]}